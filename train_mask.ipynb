{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4514173a-5735-4dc3-8076-96dfc6f9f0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-23 06:55:27.701075: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-23 06:55:27.752619: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-23 06:55:28.500314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import itertools\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "\n",
    "from data_utils import WBCdataset_Mask\n",
    "\n",
    "from transformers import ViTForImageClassification, ViTMAEConfig\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def get_WBC_transform():\n",
    "    data_transforms = []\n",
    "    data_transforms.append(transforms.Resize((224, 224)))\n",
    "    data_transforms.append(transforms.ToTensor())\n",
    "    return transforms.Compose(data_transforms)\n",
    "\n",
    "def run(device, hps):\n",
    "    wbc_subset = \"wbc100\"\n",
    "    pretrain_options = \"pRCC\"\n",
    "    use_mask=True\n",
    "    \n",
    "    out_dir = os.path.join(hps.out_dir, f'{wbc_subset}', f'{pretrain_options}')\n",
    "    if use_mask:\n",
    "        out_dir = os.path.join(out_dir, 'mask')\n",
    "    os.makedirs(out_dir, exist_ok = True)\n",
    "    writer = SummaryWriter(out_dir)\n",
    "    \n",
    "    if wbc_subset == \"wbc1\":\n",
    "        training_files = hps.WBCdata.training_files_1\n",
    "    elif wbc_subset == \"wbc10\":\n",
    "        training_files = hps.WBCdata.training_files_10\n",
    "    elif wbc_subset == \"wbc50\":\n",
    "        training_files = hps.WBCdata.training_files_50\n",
    "    else:\n",
    "        training_files = hps.WBCdata.training_files_100\n",
    "    \n",
    "    train_data = WBCdataset_Mask(training_files, hps.WBCdata.label_dict, transform=get_WBC_transform(), use_mask=use_mask, is_train=True)\n",
    "    valid_data = WBCdataset_Mask(hps.WBCdata.validation_files, hps.WBCdata.label_dict, transform=get_WBC_transform())\n",
    "    \n",
    "    label2id = {}\n",
    "    id2label = {}\n",
    "\n",
    "    for label in hps.WBCdata.label_dict.keys():\n",
    "        label2id[label] = hps.WBCdata.label_dict[label]\n",
    "        id2label[hps.WBCdata.label_dict[label]] = label\n",
    "    \n",
    "    if pretrain_options == \"pRCC\":\n",
    "        model = ViTForImageClassification.from_pretrained(\"Mo0310/vitmae_pRCC_80epochs\", \n",
    "            label2id=label2id,\n",
    "            id2label=id2label,\n",
    "            ignore_mismatched_sizes = True,\n",
    "        ).to(device)\n",
    "    elif pretrain_options == \"facebook\":\n",
    "        model = ViTForImageClassification.from_pretrained(\"facebook/vit-mae-base\", \n",
    "            label2id=label2id,\n",
    "            id2label=id2label,\n",
    "            ignore_mismatched_sizes = True,\n",
    "        ).to(device)\n",
    "    else:\n",
    "        config = ViTMAEConfig.from_pretrained(\"facebook/vit-mae-base\",\n",
    "            label2id=label2id,\n",
    "            id2label=id2label,\n",
    "            ignore_mismatched_sizes = True)\n",
    "        model = ViTForImageClassification(config).to(device)\n",
    "        \n",
    "    masked_pixel = torch.rand(1).to(device)\n",
    "    \n",
    "    train_loader = DataLoader(dataset = train_data, batch_size=hps.finetune.batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(dataset = valid_data, batch_size=hps.finetune.batch_size, shuffle=False)\n",
    "    \n",
    "    # loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # finetune optimizer\n",
    "    learnable_params = list(model.parameters())\n",
    "    learnable_params.append(masked_pixel)\n",
    "    ft_optimizer = optim.AdamW(learnable_params, lr=hps.finetune.learning_rate)\n",
    "    #ft_optimizer.param_groups.append({'params': masked_pixel })\n",
    "    # finetune scheduler\n",
    "    ft_scheduler = optim.lr_scheduler.MultiStepLR(ft_optimizer, milestones=[1, 2], gamma=hps.pretrain.lr_decay)\n",
    "    #ft_scheduler = StepLR(ft_optimizer, step_size=5, gamma=hps.finetune.lr_decay)\n",
    "    \n",
    "    for epoch in range(hps.finetune.epochs):\n",
    "        train_and_evaluate(device, epoch, model, masked_pixel, criterion, ft_optimizer, ft_scheduler, [train_loader, valid_loader], writer)\n",
    "        \n",
    "    return model\n",
    "    \n",
    "\n",
    "def train_and_evaluate(device, epoch, model, masked_pixel, criterion, optimizer, scheduler, loaders, writer):\n",
    "    train_loader, valid_loader = loaders\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "\n",
    "    for data, label, mask in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        data = data.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        p = torch.rand(1).item()\n",
    "        if p > (1/(epoch + 1)):\n",
    "            data = data * mask + masked_pixel * (1 - mask)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output.logits, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        acc = (output.logits.argmax(dim=1) == label).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss / len(train_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        epoch_val_accuracy = 0\n",
    "        epoch_val_loss = 0\n",
    "        for data, label, mask in valid_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            val_output = model(data)\n",
    "            val_loss = criterion(val_output.logits, label)\n",
    "\n",
    "            acc = (val_output.logits.argmax(dim=1) == label).float().mean()\n",
    "            epoch_val_accuracy += acc / len(valid_loader)\n",
    "            epoch_val_loss += val_loss / len(valid_loader)\n",
    "        model.train()\n",
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "    writer.add_scalar('./Loss/train', epoch_loss, epoch+1)\n",
    "    writer.add_scalar('./ACC/train', epoch_accuracy, epoch+1)\n",
    "    writer.add_scalar('./Loss/val', epoch_val_loss, epoch+1)\n",
    "    writer.add_scalar('./ACC/val', epoch_val_accuracy, epoch+1)\n",
    "    print(\n",
    "        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71e769a1-dfe0-4704-837f-541f0bd3ce6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at Mo0310/vitmae_pRCC_80epochs and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 132/132 [02:09<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 - loss : 0.2341 - acc: 0.9177 - val_loss : 0.2257 - val_acc: 0.9306\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [02:07<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 - loss : 0.0578 - acc: 0.9815 - val_loss : 0.0558 - val_acc: 0.9832\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [02:06<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 - loss : 0.0247 - acc: 0.9929 - val_loss : 0.0444 - val_acc: 0.9878\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [02:06<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 - loss : 0.0145 - acc: 0.9947 - val_loss : 0.0613 - val_acc: 0.9832\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [02:06<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5 - loss : 0.0114 - acc: 0.9963 - val_loss : 0.0489 - val_acc: 0.9838\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [02:05<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6 - loss : 0.0103 - acc: 0.9969 - val_loss : 0.0585 - val_acc: 0.9844\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [02:07<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7 - loss : 0.0056 - acc: 0.9985 - val_loss : 0.0679 - val_acc: 0.9850\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [02:06<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8 - loss : 0.0097 - acc: 0.9970 - val_loss : 0.0436 - val_acc: 0.9902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [02:05<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9 - loss : 0.0043 - acc: 0.9985 - val_loss : 0.0625 - val_acc: 0.9861\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [02:06<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10 - loss : 0.0036 - acc: 0.9987 - val_loss : 0.0837 - val_acc: 0.9826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hps = utils.get_hparams_from_file('./configs/base.json')\n",
    "seed_everything(hps.seed)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = run(device, hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea8caf3c-2d52-4cda-8ce7-9d7c8388ed5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin:   0%|          | 0.00/343M [00:00<?, ?B/s]'(MaxRetryError(\"HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/6e/bd/6ebdc123f5e327462b8fadcf0a374e7ab79b10aa5f047ca5c215db42c66495cf/9ca25b4c240189238a485f6566a71b2a15b5d146c3cfd3cb1024aac37a8be3cc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQFN2FTF47%2F20231022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231022T231819Z&X-Amz-Expires=86400&X-Amz-Signature=58d75334f3646120f7cd5e19a6b3fba4b37179d728c7cee691e1dad91832a3a8&X-Amz-SignedHeaders=host&partNumber=1&uploadId=MV4YsfW_EZHebY2fxktqZBKkTyKUyBzWNMGFRhsttRqN0CzdFkDgsCDJgdSuRomROaX5RiTD2aMnckrjIdZ03EyEeBnVhgg7enHtcqXZTE7XWtHNQwfUbSufy_zsr355&x-id=UploadPart (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))\"), '(Request ID: 3b345549-2a28-4501-9df5-9d9968b5fca8)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/6e/bd/6ebdc123f5e327462b8fadcf0a374e7ab79b10aa5f047ca5c215db42c66495cf/9ca25b4c240189238a485f6566a71b2a15b5d146c3cfd3cb1024aac37a8be3cc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQFN2FTF47%2F20231022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231022T231819Z&X-Amz-Expires=86400&X-Amz-Signature=58d75334f3646120f7cd5e19a6b3fba4b37179d728c7cee691e1dad91832a3a8&X-Amz-SignedHeaders=host&partNumber=1&uploadId=MV4YsfW_EZHebY2fxktqZBKkTyKUyBzWNMGFRhsttRqN0CzdFkDgsCDJgdSuRomROaX5RiTD2aMnckrjIdZ03EyEeBnVhgg7enHtcqXZTE7XWtHNQwfUbSufy_zsr355&x-id=UploadPart\n",
      "Retrying in 1s [Retry 1/5].\n",
      "pytorch_model.bin: 100%|██████████| 343M/343M [02:36<00:00, 2.19MB/s]    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Mo0310/5242_w_pRCC_wbc100_mask/commit/6456145d2b5eaa9836647bdb31499de4364ce0c8', commit_message='Upload ViTForImageClassification', commit_description='', oid='6456145d2b5eaa9836647bdb31499de4364ce0c8', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = \"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\"\n",
    "model.push_to_hub(\"5242_w_pRCC_wbc100_mask\", token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a57687-151a-40f9-949d-5d425abbfe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTForImageClassification(\n",
      "  (vit): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTAttention(\n",
      "            (attention): ViTSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46017abb-d613-4a3e-be42-176a2aa904d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
