{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4514173a-5735-4dc3-8076-96dfc6f9f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import itertools\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "\n",
    "from data_utils import (\n",
    "    WBCdataset,\n",
    "    pRCCdataset\n",
    ")\n",
    "from models import (\n",
    "    ViT,\n",
    "    MAE\n",
    ")\n",
    "\n",
    "WBC_mean = np.array([0.7049, 0.5392, 0.5885])\n",
    "WBC_std = np.array([0.1626, 0.1902, 0.0974])\n",
    "\n",
    "pRCC_mean = np.array([0.6843, 0.5012, 0.6436])\n",
    "pRCC_std = np.array([0.2148, 0.2623, 0.1969])\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def get_WBC_transform(is_train):\n",
    "    data_transforms = []\n",
    "    data_transforms.append(transforms.Resize((256, 256)))\n",
    "    if is_train:\n",
    "        data_transforms.append(transforms.RandomHorizontalFlip())\n",
    "    data_transforms.append(transforms.ToTensor())\n",
    "    data_transforms.append(transforms.Normalize(WBC_mean, WBC_std, inplace=True))\n",
    "    return transforms.Compose(data_transforms)\n",
    "\n",
    "def get_pRCC_transform():\n",
    "    data_transforms = []\n",
    "    data_transforms.append(transforms.CenterCrop(768))\n",
    "    data_transforms.append(transforms.RandomCrop((256, 256)))\n",
    "    # data_transforms.append(transforms.RandomHorizontalFlip())\n",
    "    data_transforms.append(transforms.ToTensor())\n",
    "    data_transforms.append(transforms.Normalize(pRCC_mean, pRCC_std, inplace=True))\n",
    "    return transforms.Compose(data_transforms)\n",
    "\n",
    "def run(device, hps):\n",
    "    pRCC_data = pRCCdataset(hps.pRCCdata.training_files, transform=get_pRCC_transform())\n",
    "    pRCC_data = torch.utils.data.Subset(pRCC_data, [0])\n",
    "    train_data = WBCdataset(hps.WBCdata.training_files_1, hps.WBCdata.label_dict, transform=get_WBC_transform(True))\n",
    "    valid_data = WBCdataset(hps.WBCdata.validation_files, hps.WBCdata.label_dict, transform=get_WBC_transform(False))\n",
    "    \n",
    "    vit = ViT(\n",
    "        image_size = hps.WBCdata.image_size,\n",
    "        patch_size = hps.WBCdata.patch_size,\n",
    "        num_classes = hps.WBCdata.num_classes,\n",
    "        pool = 'mean',\n",
    "        **hps.ViTmodel\n",
    "    ).to(device)\n",
    "    \n",
    "    mae = MAE(\n",
    "        encoder = vit,\n",
    "        **hps.MAEmodel\n",
    "    ).to(device)\n",
    "    \n",
    "    pRCC_loader = DataLoader(dataset = pRCC_data, batch_size=hps.pretrain.batch_size, shuffle=True)\n",
    "    \n",
    "    # pretrain optimizer\n",
    "    pt_optimizer = optim.Adam(mae.parameters(), lr=hps.pretrain.learning_rate)\n",
    "    # pretrain scheduler\n",
    "    pt_scheduler = StepLR(pt_optimizer, step_size=25, gamma=hps.pretrain.lr_decay)\n",
    "    \n",
    "    for epoch in tqdm(range(hps.pretrain.epochs)):\n",
    "        pretrain(device, epoch, mae, pt_optimizer, pt_scheduler, pRCC_loader)\n",
    "    \n",
    "    train_loader = DataLoader(dataset = train_data, batch_size=hps.finetune.batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(dataset = valid_data, batch_size=hps.finetune.batch_size, shuffle=True)\n",
    "    \n",
    "    # loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # finetune optimizer\n",
    "    ft_optimizer = optim.Adam(vit.parameters(), lr=hps.finetune.learning_rate)\n",
    "    # finetune scheduler\n",
    "    ft_scheduler = StepLR(ft_optimizer, step_size=1, gamma=hps.finetune.lr_decay)\n",
    "    \n",
    "    for epoch in range(hps.finetune.epochs):\n",
    "        train_and_evaluate(device, epoch, vit, criterion, ft_optimizer, ft_scheduler, [train_loader, valid_loader])\n",
    "\n",
    "\n",
    "        \n",
    "def pretrain(device, epoch, model, optimizer, scheduler, loader):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for data, *_ in loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        data = data.to(device)\n",
    "        \n",
    "        recon_loss, _ = model(data)\n",
    "        \n",
    "        recon_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss += recon_loss.item() / len(loader)\n",
    "        \n",
    "    if epoch%500 == 0:\n",
    "        img = data.cpu()[0]\n",
    "        model.eval()\n",
    "        run_one_image(device, img, model, set_name='pRCC')\n",
    "        model.train()\n",
    "        \n",
    "        print(\n",
    "        f\"Pretrain Epoch : {epoch+1} - Reconstruction loss : {epoch_loss:.4f}\\n\"\n",
    "        )\n",
    "        \n",
    "\n",
    "@torch.no_grad()\n",
    "def run_one_image(device, img, model, set_name='pRCC'):\n",
    "    assert model.training is False\n",
    "    \n",
    "    if set_name=='pRCC':\n",
    "        std = pRCC_std\n",
    "        mean = pRCC_mean\n",
    "    else:\n",
    "        raise \"NonImplemented.\"\n",
    "        \n",
    "    \n",
    "    x = img\n",
    "    img = rearrange(img, 'c h w -> h w c')\n",
    "    plt.subplot(1, 2, 1)\n",
    "    utils.show_image(img, std, mean, \"input\")\n",
    "    \n",
    "    x = x.unsqueeze(dim=0).to(device)\n",
    "    loss, y = model(x)\n",
    "    y = y.detach().cpu()[0]\n",
    "    \n",
    "    y = rearrange(y, 'c h w -> h w c')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    utils.show_image(y, std, mean, \"reconstructed\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def train_and_evaluate(device, epoch, model, criterion, optimizer, scheduler, loaders):\n",
    "    train_loader, valid_loader = loaders\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "\n",
    "    for data, label in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        acc = (output.argmax(dim=1) == label).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss / len(train_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        epoch_val_accuracy = 0\n",
    "        epoch_val_loss = 0\n",
    "        for data, label in valid_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            val_output = model(data)\n",
    "            val_loss = criterion(val_output, label)\n",
    "\n",
    "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "            epoch_val_accuracy += acc / len(valid_loader)\n",
    "            epoch_val_loss += val_loss / len(valid_loader)\n",
    "        model.train()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71e769a1-dfe0-4704-837f-541f0bd3ce6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m seed_everything(hps\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhps\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 90\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(device, hps)\u001b[0m\n\u001b[1;32m     87\u001b[0m pt_scheduler \u001b[38;5;241m=\u001b[39m StepLR(pt_optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, gamma\u001b[38;5;241m=\u001b[39mhps\u001b[38;5;241m.\u001b[39mpretrain\u001b[38;5;241m.\u001b[39mlr_decay)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(hps\u001b[38;5;241m.\u001b[39mpretrain\u001b[38;5;241m.\u001b[39mepochs)):\n\u001b[0;32m---> 90\u001b[0m     \u001b[43mpretrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpt_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpt_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpRCC_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset \u001b[38;5;241m=\u001b[39m train_data, batch_size\u001b[38;5;241m=\u001b[39mhps\u001b[38;5;241m.\u001b[39mfinetune\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     93\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset \u001b[38;5;241m=\u001b[39m valid_data, batch_size\u001b[38;5;241m=\u001b[39mhps\u001b[38;5;241m.\u001b[39mfinetune\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 117\u001b[0m, in \u001b[0;36mpretrain\u001b[0;34m(device, epoch, model, optimizer, scheduler, loader)\u001b[0m\n\u001b[1;32m    113\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    115\u001b[0m recon_loss, _ \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m--> 117\u001b[0m \u001b[43mrecon_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    119\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "hps = utils.get_hparams_from_file('./configs/base.json')\n",
    "seed_everything(hps.seed)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "run(device, hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8caf3c-2d52-4cda-8ce7-9d7c8388ed5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
